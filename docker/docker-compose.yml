services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant:/qdrant/storage
    mem_limit: 1g

  redis:
    image: redis:7
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis:/data
    mem_limit: 512m

  postgres:
    image: postgres:14
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=dify
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    mem_limit: 1g


  rag-api:
    build:
      context: ..
      dockerfile: app/rag-api/Dockerfile
    depends_on:
      - qdrant
      - ollama
    environment:
      QDRANT_URL: http://qdrant:6333
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      LLM_MODEL: ${LLM_MODEL:-gemma3:12b}
      IR_BACKEND: sqlite
      OPENSEARCH_URL: ${OPENSEARCH_URL:-https://opensearch:9200}
      OPENSEARCH_USER: ${OPENSEARCH_USER:-admin}
      OPENSEARCH_PASSWORD: ${OPENSEARCH_PASSWORD:-admin123!}
      SQLITE_PATH: /data/sqlite/ir.db
      USE_RERANK: "1"
      USE_ST: "1"
      # set to 'onnx' with RERANKER_ONNX_PATH to enable ONNX
      RERANK_BACKEND: "st"
      FEEDBACK_DIR: /data/feedback
      REDIS_URL: redis://redis:6379/0
      EMBED_CACHE: redis
      EMBED_USE_TEMPLATE: "1"
      EMBED_QUERY_PREFIX: "query: "
      EMBED_PASSAGE_PREFIX: "passage: "
      HF_HOME: /data/hf
      TRANSFORMERS_CACHE: /data/hf
    ports:
      - "8001:8000"
    volumes:
      - appdata:/data
    mem_limit: 1g
    extra_hosts:
      - "host.docker.internal:host-gateway"

  etl-api:
    build:
      context: ..
      dockerfile: app/etl-api/Dockerfile
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - STORAGE_DIR=/data/storage
      - PUBLIC_BASE_URL=http://localhost:8002
      - INTERNAL_BASE_URL=http://etl-api:8000
    ports:
      - "8002:8000"
    volumes:
      - appdata:/data
    mem_limit: 512m

  eval-api:
    build:
      context: ..
      dockerfile: app/eval-api/Dockerfile
    environment:
      - REPORTS_DIR=/data/reports
      - DATASETS_DIR=/app/datasets
      - RAG_API_BASE=http://rag-api:8000
      - JUDGE_MODEL=qwen2:32b
      - JUDGE_API=ollama
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
    ports:
      - "8003:8000"
    volumes:
      - appdata:/data
      - ../datasets:/app/datasets:ro
    mem_limit: 512m
    extra_hosts:
      - "host.docker.internal:host-gateway"

  board-api:
    build:
      context: ..
      dockerfile: app/board-api/Dockerfile
    depends_on:
      - postgres
      - etl-api
    environment:
      - DATABASE_URL=${BOARD_DATABASE_URL:-postgresql://postgres:postgres@postgres:5432/dify}
      - ETL_BASE_URL=http://etl-api:8000
    ports:
      - "8004:8000"
    mem_limit: 512m

  worker:
    build:
      context: ..
      dockerfile: app/worker/Dockerfile
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - STORAGE_DIR=/data/storage
      - SQLITE_PATH=/data/sqlite/ir.db
      - IR_BACKEND=sqlite
      - OPENSEARCH_URL=${OPENSEARCH_URL:-https://opensearch:9200}
      - OPENSEARCH_USER=${OPENSEARCH_USER:-admin}
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD:-admin123!}
      - HF_HOME=/data/hf
      - TRANSFORMERS_CACHE=/data/hf
      - USE_ST=1
    command: ["celery", "-A", "app.worker.celery_app:app", "worker", "--loglevel=info"]
    # Celery worker listening on Redis broker
    volumes:
      - appdata:/data
    mem_limit: 1g

  board:
    build:
      context: ..
      dockerfile: ui/board-react/Dockerfile
    ports:
      - "5173:80"
    mem_limit: 256m

  chatbot:
    build:
      context: ..
      dockerfile: ui/chatbot-react/Dockerfile
    ports:
      - "5174:80"
    mem_limit: 256m

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio:/data
    restart: unless-stopped
    mem_limit: 1g

  opensearch:
    build:
      context: ..
      dockerfile: docker/opensearch/Dockerfile
    profiles: ["opensearch"]
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD:-admin123!}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - opensearch:/usr/share/opensearch/data
    mem_limit: 2g

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:3
    profiles: ["opensearch"]
    environment:
      - OPENSEARCH_HOSTS=["https://opensearch:9200"]
      - OPENSEARCH_USERNAME=${OPENSEARCH_USER:-admin}
      - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD:-admin123!}
    depends_on:
      - opensearch
    ports:
      - "5601:5601"

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    environment:
      - OLLAMA_ORIGINS=*
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_LLM_LIBRARY=cublas
    volumes:
      - "${OLLAMA_MODELS_HOST_DIR:-.ollama}:/root/.ollama"
    gpus: all
    # For GPU (A100), request NVIDIA devices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Then inside container: ollama pull gemma3:12b

volumes:
  qdrant:
  redis:
  pgdata:
  appdata:
  minio:
  opensearch:
