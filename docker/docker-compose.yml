services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant:/qdrant/storage
    mem_limit: 1g

  redis:
    image: redis:7
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis:/data
    mem_limit: 512m

  postgres:
    image: postgres:14
    restart: unless-stopped
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=dify
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    mem_limit: 1g


  rag-api:
    build:
      context: ..
      dockerfile: app/rag-api/Dockerfile
    depends_on:
      - qdrant
      - ollama
    environment:
      QDRANT_URL: http://qdrant:6333
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
      SQLITE_PATH: /data/sqlite/ir.db
      USE_RERANK: "1"
      # set to 'onnx' with RERANKER_ONNX_PATH to enable ONNX
      RERANK_BACKEND: "st"
      FEEDBACK_DIR: /data/feedback
      REDIS_URL: redis://redis:6379/0
      EMBED_CACHE: redis
      EMBED_USE_TEMPLATE: "1"
      EMBED_QUERY_PREFIX: "query: "
      EMBED_PASSAGE_PREFIX: "passage: "
    ports:
      - "8001:8000"
    volumes:
      - appdata:/data
    mem_limit: 1g
    extra_hosts:
      - "host.docker.internal:host-gateway"

  etl-api:
    build:
      context: ..
      dockerfile: app/etl-api/Dockerfile
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - STORAGE_DIR=/data/storage
      - PUBLIC_BASE_URL=http://localhost:8002
      - INTERNAL_BASE_URL=http://etl-api:8000
    ports:
      - "8002:8000"
    volumes:
      - appdata:/data
    mem_limit: 512m

  eval-api:
    build:
      context: ..
      dockerfile: app/eval-api/Dockerfile
    environment:
      - REPORTS_DIR=/data/reports
      - DATASETS_DIR=/app/datasets
      - RAG_API_BASE=http://rag-api:8000
      - JUDGE_MODEL=qwen2:32b
      - JUDGE_API=ollama
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
    ports:
      - "8003:8000"
    volumes:
      - appdata:/data
    mem_limit: 512m
    extra_hosts:
      - "host.docker.internal:host-gateway"

  worker:
    build:
      context: ..
      dockerfile: app/worker/Dockerfile
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - STORAGE_DIR=/data/storage
      - SQLITE_PATH=/data/sqlite/ir.db
    command: ["celery", "-A", "app.worker.celery_app:app", "worker", "--loglevel=info"]
    # Celery worker listening on Redis broker
    volumes:
      - appdata:/data
    mem_limit: 1g

  board:
    build:
      context: ..
      dockerfile: ui/board-react/Dockerfile
    ports:
      - "5173:80"
    mem_limit: 256m

  chatbot:
    build:
      context: ..
      dockerfile: ui/chatbot-react/Dockerfile
    ports:
      - "5174:80"
    mem_limit: 256m

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio:/data
    restart: unless-stopped
    mem_limit: 1g

volumes:
  qdrant:
  redis:
  pgdata:
  appdata:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_ORIGINS=*
    volumes:
      - ${OLLAMA_MODELS_HOST_DIR:-.ollama}:/root/.ollama
    mem_limit: 2g
    # If you have GPUs, configure Docker accordingly (uncomment and adjust)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]
    # Then inside container: ollama pull gemma3:12b
  minio:
