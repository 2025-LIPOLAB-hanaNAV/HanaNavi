## Core services
REDIS_URL=redis://redis:6379/0
QDRANT_URL=http://qdrant:6333

# Dify Postgres (if you run Dify stack externally)
# NOTE: Our services (rag/etl/eval) do not use Postgres; only Dify requires it.
POSTGRES_DSN=postgresql://postgres:postgres@postgres:5432/dify

## LLM / Ollama
# If using Dockerized Ollama (default in compose), use service URL
OLLAMA_BASE_URL=http://ollama:11434
# Default model for RAG (can be overridden per-chat in UI)
LLM_MODEL=gemma3:12b
# If you want to use OpenAI-compatible provider (e.g., Dify), set:
# LLM_API=openai
# LLM_BASE_URL=http://<dify-host>:<port>
# OPENAI_API_KEY=app-xxx

# Host directory containing Ollama models to mount into the container
# Use an ABSOLUTE path. '~' is not expanded by Compose.
# Example (Linux): /home/youruser/.ollama
OLLAMA_MODELS_HOST_DIR=/home/youruser/.ollama

## Storage paths
STORAGE_DIR=/data/storage
SQLITE_PATH=/data/sqlite/ir.db

## Embedding (optional)

## IR backend
# sqlite (default) | opensearch
IR_BACKEND=sqlite
# OpenSearch connection (enable with profile: `docker compose --profile opensearch up -d`)
OPENSEARCH_URL=http://opensearch:9200
OPENSEARCH_INDEX=posts
# OPENSEARCH_USER=admin
# OPENSEARCH_PASSWORD=admin
# USE_ST=1
# EMBEDDING_MODEL=dragonkue/snowflake-arctic-embed-l-v2.0-ko
# EMBED_CACHE=redis
# EMBED_QUERY_PREFIX=query: 
# EMBED_PASSAGE_PREFIX=passage: 
